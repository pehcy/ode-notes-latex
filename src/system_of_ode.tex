\chapter{System of differential equations}

\begin{definition}
    where $\mathbf{A}(t)$ is a matrix function of the independent variable $t$.
\end{definition}

\begin{example}
    Verify that:

    \[
        \overrightarrow{\mathbf{x}}(t) = \begin{bmatrix}
            \frac{2}{3}(1 - e^{-3t})\\
            \frac{1}{3}(1 + e^{-3t})
        \end{bmatrix}
    \]

    is a solution of the system 
    \[
        \overrightarrow{\mathbf{x}}'(t) = \mathbf{A}(t)\overrightarrow{\mathbf{x}}(t) + \overrightarrow{f}(t)
    \]

    where 
    \[
        \mathbf{A}(t) = \begin{bmatrix}
            -2 & 1\\ 1 & -2
        \end{bmatrix}, \quad 
        \overrightarrow{f}(t) = \begin{bmatrix}
            1 \\ 0
        \end{bmatrix}
    \]
\end{example}
\begin{solution}
    We first differentiating the given solution $\overrightarrow{\mathbf{x}}(t)$

    \[
        \overrightarrow{\mathbf{x}}'(t) = \begin{bmatrix}
            \displaystyle \frac{d}{dt} \, \frac{2}{3}(1 - e^{-3t})\\[0.8em]
            \displaystyle \frac{d}{dt} \, \frac{1}{3}(1 + e^{-3t})
        \end{bmatrix}
        = \begin{bmatrix}
            2e^{-3t}\\ -2e^{-3t}
        \end{bmatrix} = LHS
    \]

    Now check if $\mathbf{A}(t)\overrightarrow{\mathbf{x}}(t) + \overrightarrow{f}(t)$ is equal to left-hand side.
\end{solution}

\begin{tikzpicture}[
    declare function={f(\x) = 2*\x;} % Define which function we're using
]
\begin{axis}[
    quiv, title={$\dfrac{\mathrm{d}y}{\mathrm{d}x}=2x$}
]
\addplot3 (x,y,0);
\addplot {x^2+0.15}; % You need to find the antiderivative yourself, unfortunately. Good exercise!
\end{axis}
\end{tikzpicture}

\section{Autonomous system}

Systems of linear first order ODEs with constant coefficients is an important class 
that we will discuss their solutions in detail.

\begin{example}
    Solve the system of ODEs

    \[
        \overrightarrow{\mathbf{x}}'(t) = \begin{bmatrix}
            3 & 4\\ 3 & 2
        \end{bmatrix}
        \overrightarrow{\mathbf{x}}(t) + \begin{bmatrix}
            6 \\ -12
        \end{bmatrix}
    \]
\end{example}
\begin{solution}
    Given the system of ODEs
    \begin{equation*}
        \overrightarrow{\mathbf{x}}'(t) = \begin{bmatrix}
            3 & 4\\ 3 & 2
        \end{bmatrix}
        \overrightarrow{\mathbf{x}}(t) + \begin{bmatrix}
            6 \\ -12
        \end{bmatrix} \label{eq:os1} \tag{{\color{red} $\clubsuit $}}
    \end{equation*}

    The general solution of \eqref{eq:os1} is 
    \[
        \overrightarrow{\mathbf{x}} = \overrightarrow{\mathbf{x}}_h + \overrightarrow{\mathbf{x}}_p
    \]

    First we want to find $\overrightarrow{\mathbf{x}}_h$, the homogeneous solution of 
    \[
        \overrightarrow{\mathbf{x}}'(t) = \mathbf{A}\overrightarrow{\mathbf{x}}(t)
    \]

    Find the eigenpairs of $\mathbf{A}$ by solving for eigenvalues.
    \begin{align*}
        \det (\mathbf{A} - \lambda \mathbf{I}) = 0 &\Rightarrow 
        \det \begin{bmatrix}
            3-\lambda & 4\\ 3 & 2 - \lambda
        \end{bmatrix} = 0\\
        &\Rightarrow \lambda^2 - 5\lambda - 6 = 0
    \end{align*}

    On solving, we obtain $\lambda_1 = 6$ and $\lambda_2 = -1$.

    As $\lambda_1 = 6$, we substitute $\lambda_1 = 6$ back into \eqref{eq:os1} and find an eigenvector
    \[
        \left[
        \begin{array}{cc|c}
        3-6 & 4 & 0 \\
        3 & 2-6 & 0 \\
        \end{array}
        \right] \xrightarrow[]{R_2 \leftarrow R_2 + R_1}
        \left[
        \begin{array}{cc|c}
        -3 & 4 & 0 \\
        0 & 0 & 0 \\
        \end{array}
        \right]
    \]

    which means $-3x_1 + 4x_2 = 0$. By letting $x_1 = 4$ and $x_2 = 3$ we have the first eigenvector 
    \[
        \overrightarrow{\mathbf{v}}_1 = \begin{bmatrix}
            4 \\ 3
        \end{bmatrix}
    \]

    As $\lambda_2 = -1$, we substitute $\lambda_2 = -1$ back into \eqref{eq:os1}
    \[
        \left[
        \begin{array}{cc|c}
        3+1 & 4 & 0 \\
        3 & 2+1 & 0 \\
        \end{array}
        \right] \xrightarrow[\displaystyle R_1 \leftarrow \frac{1}{4}R_1]{\displaystyle R_2 \leftarrow R_2 - \frac{3}{4}R_1}
        \left[
        \begin{array}{cc|c}
        1 & 1 & 0 \\
        0 & 0 & 0 \\
        \end{array}
        \right]
    \]

    which means $x_1 + x_2 = 0$. One of the satisfies pair is $x_1 = 1$ and $x_2 = -1$. From that we have the second eigenvector 
    \[
        \overrightarrow{\mathbf{v}}_2 = \begin{bmatrix}
            1 \\ -1
        \end{bmatrix}
    \]
    so that
    \[
        \mathbf{A} = \text{Span} \Biggl \{ \begin{bmatrix}
            3 \\ 4
        \end{bmatrix}, \begin{bmatrix}
            1 \\ -1
        \end{bmatrix}\Biggr \}
    \]
    thus the homogeneous solution of $\overrightarrow{\mathbf{x}}'(t) = \mathbf{A}\overrightarrow{\mathbf{x}}(t)$ is 
    \begin{equation*}
        \overrightarrow{\mathbf{x}}_h = c_1 \> e^{6t} \begin{bmatrix}
            4 \\ 3
        \end{bmatrix} + c_2 \> e^{-t} \begin{bmatrix}
            1 \\ -1
        \end{bmatrix}
    \end{equation*}
    where $c_1$ and $c_2$ are arbitrary constants.

    Hence we are going to use the method of undetermined coefficients to find the particular solution 
    $\overrightarrow{\mathbf{x}}_p$, we first try to set 
    \begin{equation*}
        \overrightarrow{\mathbf{x}}_p = \begin{bmatrix}
            A \\ B
        \end{bmatrix}
    \end{equation*}

    The equation is 

    \[
        \begin{bmatrix}
            3 & 4\\ 3 & 2
        \end{bmatrix}
        \begin{bmatrix}
            A \\ B
        \end{bmatrix}
        + \begin{bmatrix}
            6 \\ -12
        \end{bmatrix} = \begin{bmatrix}
            0 \\ 0
        \end{bmatrix}
    \]
    
    Solve for $A, B$.
    \[
        \left[
        \begin{array}{cc|c}
        3 & 4 & -6 \\
        3 & 2 & 12 \\
        \end{array}
        \right] \xrightarrow[R_2 \leftarrow R_2 - R_1]{}
        \left[
        \begin{array}{cc|c}
        3 & 4 & -6 \\
        0 & -2 & 18 \\
        \end{array}
        \right] \xrightarrow[\displaystyle R_1 \leftarrow \frac{R_1 - 4R_2}{3}]{\displaystyle R_2 \leftarrow -\frac{1}{2}R_2}
        \left[
        \begin{array}{cc|c}
            1 & 0 & 10 \\
            0 & 1 & -9 \\
        \end{array}
        \right]
    \]
    and we have 
    \begin{equation*}
        \overrightarrow{\mathbf{x}}_p = \begin{bmatrix}
            10 \\ -9
        \end{bmatrix}
    \end{equation*}

    Finally, the general solution for \eqref{eq:os1} is 
    \begin{equation*}
        \overrightarrow{\mathbf{x}} = c_1 \> e^{6t} \begin{bmatrix}
            4 \\ 3
        \end{bmatrix} + c_2 \> e^{-t} \begin{bmatrix}
            1 \\ -1
        \end{bmatrix}
        + \begin{bmatrix}
            10 \\ -9
        \end{bmatrix}
    \end{equation*}
\end{solution}

\subsection{Defective Matrix}

\begin{theorem}[Defective matrix]
    Assume that matrix $\mathbf{A}$ is defective with multiple eigenvalues $\lambda$ with 
    the multiplicity $m \geq 1$; The dimension of the eigenspace 

    \begin{equation}
        \text{dim}(E) = k < m \quad \text{ for some } k \in \mathbb{N}
    \end{equation}

    and the corresponding the eigenvectors in the eigenspace are $(\overrightarrow{\mathbf{v}}_1, 
    \overrightarrow{\mathbf{v}}_2, \ldots)$. Then 
\end{theorem}

\begin{algorithm}
    \caption{Finding general solution of defective matrix}\label{alg:defective}
    \KwData{$\mathbf{A}$ is $n \times n$ matrix}
    \KwResult{General solution}
    $y \gets 1$\;
    $X \gets x$\;
    $N \gets n$\;
    \While{$N \neq 0$}{
      \eIf{$N$ is even}{
        $X \gets X \times X$\;
        $N \gets \frac{N}{2}$ \Comment*[r]{This is a comment}
      }{\If{$N$ is odd}{
          $y \gets y \times X$\;
          $N \gets N - 1$\;
        }
      }
    }
\end{algorithm}

\begin{example}
    Solve the system 

    \begin{equation*}
        \overrightarrow{\mathbf{x}}' = \mathbf{A}\overrightarrow{\mathbf{x}} \label{eq:os3.0} \tag{{\color{black} $\spadesuit $}}
    \end{equation*}

    with
    
    \[
        \mathbf{A} = \begin{bmatrix}
            2 & 1 & 6\\ 0 & 2 & 5\\ 0 & 0 & 2
        \end{bmatrix}
    \]
\end{example}
\begin{solution}
    By solving $(\mathbf{A} - \lambda \mathbf{I}) = 0$, we obtained $\lambda = 2$ with multiplicity of 3.
    Here we only get one LI eigenvector which is 

    \[
        \overrightarrow{\mathbf{v}}_1 = \begin{bmatrix}
            1 & 0 & 0
        \end{bmatrix} ^ T
    \]

    Since $\mathbf{A}$ is $3\times 3$ square matrix, we need to find \textbf{two} more LI eigenvectors. Suppose 
    that $\overrightarrow{\mathbf{v}}_2, \overrightarrow{\mathbf{v}}_3$ are LI eigenvectors. For this we need to solve 

    \begin{align*}
        &(\mathbf{A} - \lambda \mathbf{I})\overrightarrow{\mathbf{v}}_2 = \overrightarrow{\mathbf{v}}_1 \label{eq:os3.1} \tag{{\color{cyan} $\blacklozenge$}} \\
        &(\mathbf{A} - \lambda \mathbf{I})\overrightarrow{\mathbf{v}}_3 = \overrightarrow{\mathbf{v}}_2 \label{eq:os3.2} \tag{{\color{orange} $\bigstar  $}}
    \end{align*}

    Solve \eqref{eq:os3.2}, 
    \begin{equation*}
        \left[
        \begin{array}{ccc|c}
        2-2 & 1 & 6 & 0\\
        0 & 2-2 & 5 & 1\\
        0 & 0 & 2-2 & 0
        \end{array}
        \right] \xrightarrow[]{}
        \left[
        \begin{array}{ccc|c}
            0 & 1 & 6 & 0\\
            0 & 0 & 5 & 1\\
            0 & 0 & 0 & 0
        \end{array}
        \right]
    \end{equation*}

    here we have
    \[
        5x_3 = 1 \Rightarrow x_3 = \frac{1}{5}
    \]
    \[
        x_2 = -6x_3 = -\frac{6}{5}
    \]
    \[
        x_1 = 0
    \]
    we set $x_1 = t$ as free variable, the general solution of \eqref{eq:os3.0} is 

    \[
        \overrightarrow{\mathbf{x}} = c_1 \overrightarrow{\mathbf{v}}_1 + c_2 [\overrightarrow{\mathbf{v}}_1 t\, e^{2t} + \overrightarrow{\mathbf{v}}_2 t\, e^{2t}]
        + c_3 \big[ \frac{1}{2} \overrightarrow{\mathbf{v}}_1 \, t^2 \, e^{2t} + \overrightarrow{\mathbf{v}}_2 t\, e^{2t} + \overrightarrow{\mathbf{v}}_3 e^{2t} \big]
    \]

    where $c_1, c_2$ and $c_3$ are arbitrary constants.
\end{solution}

\section{Non-linear system of differential equations}

\subsection{Tangent plane}

Intuitively, it seems clear that in a continuous and smooth plane in 3-dimensional space. There are many straight lines 
that can be tangent to a given point $\Gamma_0$.

\begin{center}
    \begin{tikzpicture}[font=\sffamily,declare function={%
        f(\x,\y)=3+.075*cos(\x*100)*cos(\y*100)-.035*(\y-5)^2-.01*(\x-5)^2;
        g(\x) = .075*(\x-4)^2+4;
        h(\x) = .02*(\x-4)^3-.2*(\x-4)+4;
        t(\x,\y)=f(4,4) -0.05*(\x-4) + 0.02*(\y-4); }]
    
        \tdplotsetmaincoords{70}{110}
    
        \begin{scope}[tdplot_main_coords]
            \draw[-latex] (0,0,0) -- (8,0,0) node[anchor=north] {$x$};
            \draw[-latex] (0,0,0) -- (0,8,0) node[anchor=west] {$y$};
            \draw[-latex] (0,0,0) -- (0,0,4) node[anchor=south] {$z=f(x, y)$};
    
            %Grid       
            \foreach \X in {1,...,6}
            {
                \draw[thin,gray!40] plot[variable=\x,samples=60,smooth,domain=1:6] ({\X},{\x},0);
                \draw[thin,gray!40] plot[variable=\x,samples=60,smooth,domain=1:6] ({\x},{\X},0);
                \draw[thin,gray!40] plot[variable=\x,samples=60,smooth,domain=1:6] ({\X},{\x},{f(\X,\x)});
                \draw[thin,gray!40] plot[variable=\x,samples=60,smooth,domain=1:6] ({\x},{\X},{f(\x,\X)});
            }     
    
            % Curves
            \draw[darkolivegreen,thick] plot[variable=\x,samples=60,smooth,domain=1:6]
            ({g(\x)},{\x},{f(g(\x),\x)}) node [right] {\scriptsize $f_x$};
    
            \draw[darkolivegreen,thick] plot[variable=\x,samples=60,smooth,domain=1:6]
            ({g(\x)},{\x},0) node [right] {\scriptsize $y(t)$};
    
            \draw[orange,thick] plot[variable=\x,samples=60,smooth,domain=1:6]
            ({\x},{h(\x)},0) node [below] {\scriptsize $x(t)$};
    
            \draw[orange,thick] plot[variable=\x,samples=60,smooth,domain=1:6]
            ({\x},{h(\x)},{f(\x,h(\x))});
    
            \node[orange, anchor=west] at ({0},{h(0)},{f(0,h(0))}) {\scriptsize $f_y$};
    
            % Tangent plane
            \fill[purple!10,opacity=0.4,draw=purple] (3,3,{t(3,3)}) -- (3,5,{t(3,5)}) -- (5,5,{t(5,5)}) -- (5,3,{t(5,3)}) -- (3,3,{t(3,3)}) node[anchor=north west,purple,opacity=1] {\scriptsize $\Gamma_0$};
    
            % Vector fields
            \draw[->,>=stealth,thick,blue] (4,4,{f(4,4)}) -- (4,5.02,{f(4,4)+.06}) node [below] {\scriptsize $N(x_0)$};
            \draw[->,>=stealth,red,thick] (4,4,0) -- (3,4.2,0) node[anchor=west] {};
            \draw[->,>=stealth,red,thick] (4,4,{f(4,4)}) -- (3,4.2,{f(4,4)+.06}) node[anchor=south east] {};
    
            % Points
            \fill[black] (4,4,0) circle (1pt) node[anchor=north] {\scriptsize $(x_0, y_0)$};
            \fill[black] (4,4,{f(4,4)}) circle (1pt) node[anchor=north east] {};
        \end{scope}
    \end{tikzpicture}
\end{center}

\begin{theorem}[Linearlization of a function]
    Let $P_0 = (x_0, y_0)$ be a point on a surface $f$. Then the linearization of $f(x, y)$ 
    at point $P_0$ is 
    
    \begin{equation}
        L(x,y) = f(x_0, y_0) + f_x(x_0, y_0)(x - x_0) + f_y(x_0, y_0)(y - y_0) 
    \end{equation}

    where $f_x$ and $f_y$ are the partial derivative of $f$ with $x$ and $y$ respectively.
\end{theorem}

\begin{example}
    Find the critical points of 
    \begin{equation*}
        \begin{cases}
            x' = x(1-y)\\
            y' = y(2-x)
        \end{cases}
    \end{equation*}

    and hence solve the system.
\end{example}
\begin{solution}
    We can rewrite the original DE to
    \begin{equation*}
        \begin{cases}
            x' = x-xy\\
            y' = 2y-xy
        \end{cases} \label{eq:os2.1} \tag{{\color{red} $\heartsuit  $}}
    \end{equation*}

    and with that, we can express it in the vector form
    \[
        \overrightarrow{\mathbf{x}}'(x) = \mathbf{A}\overrightarrow{\mathbf{x}} + \overrightarrow{G}(x,y)
    \]
    
    where
    \[
        \mathbf{A} = \begin{bmatrix}
            1 & 0\\ 0 & 2
        \end{bmatrix}, \quad \text{ and }
        \overrightarrow{G}(x,y) = \begin{bmatrix}
            -xy \\ -xy 
        \end{bmatrix}
        = \begin{bmatrix}
            f(x,y) \\ g(x,y)
        \end{bmatrix}
    \]

    We want to show that \eqref{eq:os2.1} is almost linear near the origin $(0,0)$.
    
    \textbf{Intuition}: We can try to show that
    \begin{equation*}
        \lim_{\norm{x} \to 0} \frac{\norm{\overrightarrow{G}(x,y)}}{\norm{x}} = 0
    \end{equation*}

    Compute the limit
    \begin{equation*}
        \lim_{(x,y) \to (0, 0)} \frac{f(x,y)}{\sqrt{x^2 + y^2}} = \lim_{(x,y) \to (0, 0)} \frac{-xy}{\sqrt{x^2 + y^2}} 
        \label{eq:os2.2} \tag{{\color{black} $\clubsuit$}}
    \end{equation*}

    It is a bit tricky to compute this limit. However, we can compute the limit by converting them into a cylindrical coordinates system 
    by substituting $x = \cos \theta$ and $y = \sin \theta$ into \eqref{eq:os2.2}. Now it becomes

    \begin{equation*}
        \eqref{eq:os2.2} \Rightarrow \lim_{r \to 0^+} \frac{-r^2 \cos \theta \sin \theta}{\sqrt{r^2(\cos^2 \theta + \sin^2 \theta)}} = \lim_{r \to 0^+} -r(\underbrace{\cos \theta \sin \theta}_\text{finite}) = 0
    \end{equation*}

    Since $f(x,y) = g(x,y) = -xy$, it is certain that 
    \begin{equation*}
        \lim_{(x,y) \to (0, 0)} \frac{g(x,y)}{\sqrt{x^2 + y^2}} = 0
    \end{equation*}

    thus, we conclude that \eqref{eq:os2.1} is almost linear at $(0,0)$. Now we are beginning to work for the 
    solution of this system.

    By inspection on \eqref{eq:os2.1}, we took an initial guess of $x = 0$ or $y = 1$ are critical points.

    \textbf{Case 1}: If $x = 0$, then $2y = 0 \Rightarrow y =0$.

    \textbf{Case 2}: If $y = 0$, $(2, 1)$ is also a critical point.
\end{solution}

\section{Variation of Parameters}

\subsection{Fundamental matrix}

We return to the system 

\begin{equation}
    \overrightarrow{\mathbf{x}}' = \mathbf{A}(t) \overrightarrow{\mathbf{x}}
\end{equation}

with the general solution 

\begin{equation}
    \overrightarrow{\mathbf{x}} = c_1 \overrightarrow{\mathbf{x}}_1(t) + c_2 \overrightarrow{\mathbf{x}}_2(t) 
    + \cdots + c_n \overrightarrow{\mathbf{x}}_n(t)
\end{equation}

where $\overrightarrow{\mathbf{x}}_1, \overrightarrow{\mathbf{x}}_2, \ldots, \overrightarrow{\mathbf{x}}_n$ are 
$n$ independent solutions to the system, and $c_1, c_2, \ldots, c_n$ are distinct constants.

We form the matrix whose columns are the solutions $\overrightarrow{\mathbf{x}}_1, \overrightarrow{\mathbf{x}}_2, \ldots, \overrightarrow{\mathbf{x}}_n$:

\begin{equation}
    \Phi(t) = \begin{bmatrix}
        \overrightarrow{\mathbf{x}}_1 & \overrightarrow{\mathbf{x}}_2 & \ldots & \overrightarrow{\mathbf{x}}_n
    \end{bmatrix}
\end{equation}

Since the solutions are linearly independent, we can called them a \textbf{fundamental} set of solutions, and the matrix 
whose columns are linearly independent and fundamental is called a \textbf{fundamental matrix} for the system. 

We can writing the general solution of homogeneous system using $\Phi(t)$. As a first application of $\Phi(t)$, we can use 
it to write the general solution 

\begin{equation}
    \overrightarrow{\mathbf{x}} = c_1 \overrightarrow{\mathbf{x}}_1(t) + c_2 \overrightarrow{\mathbf{x}}_2(t) + 
    \cdots + c_n \overrightarrow{\mathbf{x}}_n(t) 
\end{equation}

efficiently, which becomes using the fundamental matrix
\begin{equation}
    \overrightarrow{\mathbf{x}} = \Phi(t) \mathbf{c} \quad \text{where } \mathbf{c} = (c_1, c_2, \ldots, c_n)^T
\end{equation}

Note that the constants $\mathbf{c}$ must but located on the right, even though the $c$'s are usually written 
on the left when they are the coefficients of the solution $\overrightarrow{\mathbf{x}}_i$.

\begin{theorem}[Fundamental matrix]
    $\Phi(t)$ is a fundamental matrix for the system 
    \begin{equation}
        \overrightarrow{\mathbf{x}}' = \mathbf{A} \overrightarrow{\mathbf{x}}
    \end{equation}
    
    if $\det \Phi(t) \neq 0$ and it satisfies the matrix equation
    \begin{equation}
        \Phi' = \mathbf{A}\Phi
    \end{equation}

    where $\Phi'$ means that each entry of $\Phi$ has been differentiated.
\end{theorem}

\begin{theorem}[Variation of parameters with fundamental matrix]
    The particular solution for the system $\overrightarrow{\mathbf{x}}'(t) = \mathbf{A} \overrightarrow{\mathbf{x}}$ is 

    \begin{equation}
        \overrightarrow{\mathbf{x}}_p(t) = \Phi (t) \int \Phi(s)^{-1}\, \overrightarrow{f}(t) \> dt
    \end{equation}

    where $\Phi(t)$ is a fundamental matrix, and $\Phi^{-1}(t)$ is the inverse of fundamental matrix.
\end{theorem}

\begin{example}
    Find a particular solution to the system
    \begin{equation*}
        \overrightarrow{\mathbf{x}}' = \mathbf{A}\overrightarrow{\mathbf{x}} + \overrightarrow{f}(t) 
    \end{equation*}
    
    where 
    \[
        \mathbf{A} = \begin{bmatrix}
            1 & 4\\ 1 & 4
        \end{bmatrix}, \quad 
        \overrightarrow{f}(t) = \begin{bmatrix}
            2e^{3t} \\ 0
        \end{bmatrix}
    \]

    Given that the complementary solution is 
    \[
        \overrightarrow{\mathbf{x}}_h = c_1\, e^{3t} \begin{bmatrix}
            1 \\ 2
        \end{bmatrix} + c_2\, e^{-t} \begin{bmatrix}
            1 \\ -2
        \end{bmatrix}
    \]
\end{example}
\begin{solution}
    From the given $\overrightarrow{\mathbf{x}}_h$, we know that the fundamental solutions are 
    \[
        \overrightarrow{\mathbf{x}}_1 = e^{3t} \begin{pmatrix}
            1 & 2
        \end{pmatrix}^T, \quad 
        \overrightarrow{\mathbf{x}}_2 = e^{-t} \begin{pmatrix}
            1 & -2
        \end{pmatrix}^T
    \]
    
    By using the method of parameters, the fundamental matrix is 
    \begin{equation}
        \Phi(t) = \begin{bmatrix}
            e^{3t} & e^{-t}\\ 2e^{3t} & -2e^{-t}
        \end{bmatrix}
    \end{equation}

    and the inverse is
    \begin{equation}
        \Phi(t)^{-1} = -\, \frac{1}{4e^{2t}} \begin{bmatrix}
            -2e^{-t} & -e^{-t}\\ -2e^{3t} & e^{3t}
        \end{bmatrix}
    \end{equation}

    compute 
    \begin{align*}
        \Phi(t)^{-1} \overrightarrow{f}(t) &= -\, \frac{1}{4e^{2t}} \begin{bmatrix}
            -2e^{-t} & -e^{-t}\\ -2e^{3t} & e^{3t}
        \end{bmatrix}
        \begin{bmatrix}
            2e^{3t} \\ 0
        \end{bmatrix} \\
        &= -\, \frac{1}{4e^{2t}} \begin{bmatrix}
            -4e^{2t} \\ -4e^{6t}
        \end{bmatrix} \\
        &= \begin{bmatrix}
            1 \\ e^{4t}
        \end{bmatrix}
    \end{align*}

    now we continue and find $\overrightarrow{\mathbf{x}}_p$ using the formula
    \begin{align*}
        \overrightarrow{\mathbf{x}}_p = \Phi (t) \int \Phi^{-1} (t) \overrightarrow{f}(t) \> dt
        &= \begin{bmatrix}
            e^{3t} & e^{-t}\\ 2e^{3t} & -2e^{-t}
        \end{bmatrix} \, \int \begin{bmatrix}
            1 \\ e^{4t}
        \end{bmatrix} \> dt \\[0.8em]
        &= \begin{bmatrix}
            e^{3t} & e^{-t}\\ 2e^{3t} & -2e^{-t}
        \end{bmatrix} \, \begin{bmatrix}
            \int 1 \> dt \\[0.6em]  \int e^{4t} \> dt
        \end{bmatrix}\\
        &= \begin{bmatrix}
            e^{3t} & e^{-t}\\ 2e^{3t} & -2e^{-t}
        \end{bmatrix} \, \begin{bmatrix}
            t \\ \frac{1}{4}e^{4t} 
        \end{bmatrix} & \text{ignore constants}\\
    \end{align*}

    thus the particular solution is 
    \[
        \overrightarrow{\mathbf{x}}_p = e^{3t} \begin{bmatrix}
            t + \frac{1}{4} \\[0.6em] 2t - \frac{1}{2}
        \end{bmatrix}
    \]
\end{solution}

\begin{theorem}[Solving IVP using fundamental matrix]
    The solution to the IVP 

    \begin{equation}
        \overrightarrow{\mathbf{x}}' = \mathbf{A}(t) \overrightarrow{\mathbf{x}}, \quad \overrightarrow{\mathbf{x}}(t_0) = \overrightarrow{\mathbf{x}}_0 
    \end{equation}

    can now be written as

    \begin{equation}
        \overrightarrow{\mathbf{x}}_p(t) = \Phi (t) \Phi (t_0)^{-1} \overrightarrow{\mathbf{x}}_0 + \Phi (t) \int^t_0 \Phi^{-1} (s) \overrightarrow{f}(s) \> ds
    \end{equation}

    where $\Phi(t)$ is a fundamental matrix, and $\Phi^{-1}(t)$ is the inverse of fundamental matrix.
\end{theorem}

\section{Tutorials}

\begin{mdframed}
    \vspace{-0.25cm}
    \hspace{-0.25cm}
    \begin{Exercise}
        Find a general solution to 

        \[
            \overrightarrow{\mathbf{x}}'(t) = \begin{bmatrix}
                -3 & 1 \\ 2 & -2
            \end{bmatrix} \overrightarrow{\mathbf{x}}(t) + \begin{bmatrix}
                0 \\ 2
            \end{bmatrix} \cos 2t
        \]
    \end{Exercise}

    \begin{Exercise}
        Solve the system

        \[
            \overrightarrow{\mathbf{x}}'(t) = \begin{bmatrix}
                0 & 2 & -3 \\ -2 & 4 & -3 \\ -2 & 2 & -1
            \end{bmatrix} \overrightarrow{\mathbf{x}}(t) + \begin{bmatrix}
                0 \\ 0 \\ e^t
            \end{bmatrix}
        \]
    \end{Exercise}


    \begin{Exercise}
        Consider the autonomous system

        \[
            \begin{cases}
                x' = x(1 - x - y)\\
                y' = \displaystyle y\biggl(\frac{3}{4} - y -\frac{1}{2}x \biggr)
            \end{cases}
        \]

        \begin{enumerate}
            \item Find the critical points of the system above.
            \item Find the linearization at each of the the critical points above.
            \item Sketch the phase potrait for the autonomous system above.
        \end{enumerate}
    \end{Exercise}
\end{mdframed}